# üöÄ Big Data Ateliers ‚Äì Complete Hadoop Ecosystem

<p align="center">  
  <img src="https://img.shields.io/badge/Docker-ready-blue?logo=docker"/>  
  <img src="https://img.shields.io/badge/Hadoop-3.x-yellow?logo=apache"/>  
  <img src="https://img.shields.io/badge/Hive-SQL-orange?logo=apachehive"/>  
  <img src="https://img.shields.io/badge/Pig-Scripting-lightgrey?logo=apache"/>  
</p>  

<p align="center">  
A complete <b>Hadoop ecosystem lab</b> with <b>6 ateliers</b> to practice HDFS, MapReduce, Pig, Hive, file formats, and partitioning.  
Everything runs in <b>Docker</b> for simplicity and reproducibility.  
</p>  

---

## üåê Architecture

```mermaid
flowchart TD
    Client[üíª Client] --> NN[Namenode]
    NN -->|Stores metadata| DNs[Datanodes]
    DNs -->|Stores blocks| HDFS[(HDFS Storage)]
    HDFS --> MR[‚öôÔ∏è MapReduce]
    MR --> Pig[üêñ Pig Scripts]
    MR --> Hive[üêù Hive SQL]
```

---

## ‚ö° Quick Start

<details>  
<summary>üîß Setup Instructions</summary>  

```bash
# Clone repo
git clone https://github.com/your-org/hadoop-ecosystem-ateliers.git
cd hadoop-ecosystem-ateliers

# Build and start cluster
docker compose up -d --build

# Enter namenode
docker exec -it namenode bash

# Prepare ateliers in HDFS
hdfs dfs -mkdir -p /ateliers/{atelier1,atelier2,atelier3,atelier4,atelier5,atelier6}

# Upload datasets
for i in {1..6}; do
  hdfs dfs -put /ateliers_local/atelier$i/* /ateliers/atelier$i/
done
```

</details>  

---

# üìö Ateliers

---

## 1Ô∏è‚É£ Atelier 1 ‚Äì HDFS Basics

<p align="center"><b>Explore HDFS commands and block reports.</b></p>

```bash
# List ateliers
hdfs dfs -ls /ateliers/

# Show contents
hdfs dfs -cat /ateliers/atelier1/input/FL_insurance.csv | head -3

# Report
hdfs dfsadmin -report | head -20
```

‚úÖ **Goal**: Learn file navigation, inspection, and block reports.

---

## 2Ô∏è‚É£ Atelier 2 ‚Äì MapReduce

<p align="center"><b>Run your first MapReduce WordCount job.</b></p>

```bash
hdfs dfs -mkdir -p /ateliers/atelier2/wordcount/{input,output}
hdfs dfs -put /ateliers/atelier2/maman.txt /ateliers/atelier2/wordcount/input/

# Compile & package
javac -cp $(hadoop classpath) -d build WordCount.java
jar -cvf wordcount.jar -C build/ .

# Run job
hadoop jar wordcount.jar WordCount \
  /ateliers/atelier2/wordcount/input \
  /ateliers/atelier2/wordcount/output
```

‚úÖ **Goal**: Understand compilation, packaging, and execution of MapReduce jobs.

---

## 3Ô∏è‚É£ Atelier 3 ‚Äì Pig

<p align="center"><b>Analytics with Pig scripting.</b></p>

```bash
pig -x local << 'EOF'
vols = LOAD '/ateliers/atelier3/vol.csv' USING PigStorage(';') 
       AS (year:int, month:int, day:int, flight:chararray, depart:chararray, destination:chararray, distance:int);

departure_counts = FOREACH (GROUP vols BY depart) GENERATE group, COUNT(vols);
STORE departure_counts INTO '/ateliers/atelier3/pig_output/departure_counts';
quit;
EOF
```

‚úÖ **Goal**: Aggregations with Pig Latin.

---

## 4Ô∏è‚É£ Atelier 4 ‚Äì Hive Basics

<p align="center"><b>Query datasets with SQL using Hive.</b></p>

```sql
CREATE DATABASE IF NOT EXISTS atelier4;
USE atelier4;

CREATE EXTERNAL TABLE vols (
    year INT, month INT, day INT, flight STRING,
    depart STRING, destination STRING, distance INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'
LOCATION '/ateliers/atelier4/';

-- Flights per airport
SELECT depart, COUNT(*) FROM vols GROUP BY depart LIMIT 10;
```

‚úÖ **Goal**: Create and query Hive tables.

---

## 5Ô∏è‚É£ Atelier 5 ‚Äì Hive File Formats

<p align="center"><b>Compare performance of text vs Parquet.</b></p>

```sql
CREATE EXTERNAL TABLE bank_txt (
    id INT, age INT, sex STRING, region STRING, income DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/ateliers/atelier5/';

CREATE TABLE bank_parquet STORED AS PARQUET AS SELECT * FROM bank_txt;

-- Compare performance
SELECT region, AVG(income) FROM bank_txt GROUP BY region;
```

‚úÖ **Goal**: Learn storage formats (Text vs Parquet).

---

## 6Ô∏è‚É£ Atelier 6 ‚Äì Hive Partitioning

<p align="center"><b>Optimize queries with partitioned tables.</b></p>

```sql
CREATE TABLE bank_partitioned (
    id INT, age INT, sex STRING, income DOUBLE
)
PARTITIONED BY (region STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

-- Dynamic partitioning
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE bank_partitioned PARTITION(region)
SELECT id, age, sex, income, region FROM atelier5.bank_txt;
```

‚úÖ **Goal**: Learn Hive partitioning for performance.

---

# ‚úÖ Progress Tracker

<p align="center">  
<table>  
<tr><th>Atelier</th><th>Status</th></tr>  
<tr><td>HDFS</td><td>‚úÖ Completed</td></tr>  
<tr><td>MapReduce</td><td>‚úÖ Completed</td></tr>  
<tr><td>Pig</td><td>‚úÖ Completed</td></tr>  
<tr><td>Hive Basics</td><td>‚úÖ Completed</td></tr>  
<tr><td>Hive File Formats</td><td>‚úÖ Completed</td></tr>  
<tr><td>Hive Partitioning</td><td>‚úÖ Completed</td></tr>  
</table>  
</p>  

---

<p align="center"><i>üéâ Congratulations! You now have a fully running Hadoop ecosystem with all 6 ateliers explored.</i></p>  
