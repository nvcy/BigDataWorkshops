# ğŸš€ Big Data Ateliers â€“ Hadoop Ecosystem Labs

[![Docker](https://img.shields.io/badge/Docker-ready-blue?logo=docker)](https://www.docker.com/)
[![Hadoop](https://img.shields.io/badge/Hadoop-3.x-yellow?logo=apache)](https://hadoop.apache.org/)
[![Hive](https://img.shields.io/badge/Hive-SQL%20on%20HDFS-orange?logo=apachehive)](https://hive.apache.org/)
[![Pig](https://img.shields.io/badge/Pig-Scripting%20on%20HDFS-lightgrey)](https://pig.apache.org/)

ğŸ“Œ This repository contains **6 hands-on Ateliers** to explore the Hadoop ecosystem inside Docker:

* HDFS commands
* MapReduce programming
* Pig scripting
* Hive queries
* Hive file formats (Text/Parquet)
* Hive partitioning

---

## ğŸ“Š Ecosystem Architecture

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Client  â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   Namenode    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â–¼                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Datanodes â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚   HDFS Data  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ MapReduce   â”‚  â† WordCount, custom jobs
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     Pig     â”‚     Hive     â”‚
   â”‚ (scripts)   â”‚ (SQL engine) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Quick Setup

```bash
# Enter namenode container
docker exec -it namenode bash

# Create atelier directories
hdfs dfs -mkdir -p /ateliers/{atelier1,atelier2,atelier3,atelier4,atelier5,atelier6}

# Upload datasets
for i in {1..6}; do
  hdfs dfs -put /opt/scripts/../data/ateliers/atelier$i/* /ateliers/atelier$i/
done
```

---

## ğŸ“‚ Ateliers

### ğŸ“ Atelier 1 â€“ HDFS Commands

<details>
<summary>ğŸ” Click to expand</summary>  

```bash
# List ateliers
hdfs dfs -ls /ateliers/

# Show file contents
hdfs dfs -cat /ateliers/atelier1/input/FL_insurance.csv | head -3

# HDFS report
hdfs dfsadmin -report | head -20
```

âœ… Covers: HDFS navigation, file ops, block info.

</details>  

---

### ğŸ“ Atelier 2 â€“ MapReduce

<details>
<summary>ğŸ“ WordCount Job</summary>  

```bash
hdfs dfs -mkdir -p /ateliers/atelier2/wordcount/{input,output}
hdfs dfs -put /ateliers/atelier2/maman.txt /ateliers/atelier2/wordcount/input/

# Compile WordCount.java
javac -cp $(hadoop classpath) -d build WordCount.java
jar -cvf wordcount.jar -C build/ .

# Run job
hadoop jar wordcount.jar WordCount \
  /ateliers/atelier2/wordcount/input \
  /ateliers/atelier2/wordcount/output
```

âœ… Covers: MapReduce basics, WordCount results.

</details>  

---

### ğŸ“ Atelier 3 â€“ Pig

<details>
<summary>ğŸ– Pig Analytics</summary>  

```bash
pig -x local << 'EOF'
vols = LOAD '/ateliers/atelier3/vol.csv' USING PigStorage(';') 
       AS (year:int, month:int, day:int, flight:chararray, depart:chararray, destination:chararray, distance:int);

departure_counts = FOREACH (GROUP vols BY depart) GENERATE group, COUNT(vols);
STORE departure_counts INTO '/ateliers/atelier3/pig_output/departure_counts';
quit;
EOF
```

âœ… Covers: Pig scripting, grouping, aggregations.

</details>  

---

### ğŸ“ Atelier 4 â€“ Hive Basics

<details>
<summary>ğŸ SQL on Hadoop</summary>  

```sql
CREATE DATABASE IF NOT EXISTS atelier4;
USE atelier4;

CREATE EXTERNAL TABLE vols (
    year INT, month INT, day INT, flight STRING,
    depart STRING, destination STRING, distance INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'
LOCATION '/ateliers/atelier4/';

-- Query: flights per airport
SELECT depart, COUNT(*) FROM vols GROUP BY depart LIMIT 10;
```

âœ… Covers: Hive tables, queries, aggregations.

</details>  

---

### ğŸ“ Atelier 5 â€“ Hive File Formats

<details>
<summary>ğŸ“¦ Text vs Parquet</summary>  

```sql
CREATE EXTERNAL TABLE bank_txt (
    id INT, age INT, sex STRING, region STRING, income DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/ateliers/atelier5/';

CREATE TABLE bank_parquet STORED AS PARQUET AS SELECT * FROM bank_txt;

-- Compare queries
SELECT region, AVG(income) FROM bank_txt GROUP BY region;
```

âœ… Covers: Storage optimization, Parquet vs Text.

</details>  

---

### ğŸ“ Atelier 6 â€“ Hive Partitioning

<details>
<summary>ğŸ“Š Partitioned Tables</summary>  

```sql
CREATE TABLE bank_partitioned (
    id INT, age INT, sex STRING, income DOUBLE
)
PARTITIONED BY (region STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

-- Load with dynamic partitioning
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE bank_partitioned PARTITION(region)
SELECT id, age, sex, income, region FROM atelier5.bank_txt;
```

âœ… Covers: Partitioning, query optimization.

</details>  

---

## âœ… Final Checklist

* [x] HDFS commands (Atelier 1)
* [x] MapReduce WordCount (Atelier 2)
* [x] Pig analytics (Atelier 3)
* [x] Hive basics (Atelier 4)
* [x] Hive file formats (Atelier 5)
* [x] Hive partitioning (Atelier 6)

ğŸ‰ All ateliers completed successfully!

---

ğŸ”— **Tip:** You can re-run any atelier independently by entering the container:

```bash
docker exec -it namenode bash
```

---
