# ğŸš€ Big Data Ateliers â€“ Complete Hadoop Ecosystem

<p align="center">  
  <img src="https://img.shields.io/badge/Docker-ready-blue?logo=docker"/>  
  <img src="https://img.shields.io/badge/Hadoop-3.x-yellow?logo=apache"/>  
  <img src="https://img.shields.io/badge/Hive-SQL-orange?logo=apachehive"/>  
  <img src="https://img.shields.io/badge/Pig-Scripting-lightgrey?logo=apache"/>  
</p>  

<p align="center">  
A complete **Hadoop ecosystem lab** with **6 ateliers** to practice:  
ğŸ”¹ HDFS  
ğŸ”¹ MapReduce  
ğŸ”¹ Pig  
ğŸ”¹ Hive  
ğŸ”¹ File formats  
ğŸ”¹ Partitioning  

All running in **Docker** for simplicity and reproducibility.

</p>  

---

## ğŸŒ Architecture

```mermaid
flowchart TD
    Client[ğŸ’» Client] --> NN[Namenode]
    NN -->|Stores metadata| DNs[Datanodes]
    DNs -->|Stores blocks| HDFS[(HDFS Storage)]
    HDFS --> MR[âš™ï¸ MapReduce]
    MR --> Pig[ğŸ– Pig Scripts]
    MR --> Hive[ğŸ Hive SQL]
```

---

## âš¡ Quick Start

```bash
# Clone repository
git clone https://github.com/your-org/hadoop-ecosystem-ateliers.git
cd hadoop-ecosystem-ateliers

# Build and start cluster
docker compose up -d --build

# Access Namenode
docker exec -it namenode bash

# Prepare ateliers in HDFS
hdfs dfs -mkdir -p /ateliers/{atelier1,atelier2,atelier3,atelier4,atelier5,atelier6}

# Upload datasets
for i in {1..6}; do
  hdfs dfs -put /ateliers_local/atelier$i/* /ateliers/atelier$i/
done
```

---

## ğŸ“š Ateliers

### 1ï¸âƒ£ Atelier 1 â€“ HDFS Basics

<details>
<summary>ğŸ” Explore HDFS</summary>

```bash
# List ateliers
hdfs dfs -ls /ateliers/

# Show contents
hdfs dfs -cat /ateliers/atelier1/input/FL_insurance.csv | head -3

# Cluster report
hdfs dfsadmin -report | head -20
```

âœ… Learn file navigation, inspection, and block reports.

</details>

---

### 2ï¸âƒ£ Atelier 2 â€“ MapReduce

<details>
<summary>âš™ï¸ Run WordCount Job</summary>

```bash
# Create input/output dirs
hdfs dfs -mkdir -p /ateliers/atelier2/wordcount/{input,output}
hdfs dfs -put /ateliers/atelier2/maman.txt /ateliers/atelier2/wordcount/input/

# Compile & package
javac -cp $(hadoop classpath) -d build WordCount.java
jar -cvf wordcount.jar -C build/ .

# Run job
hadoop jar wordcount.jar WordCount \
  /ateliers/atelier2/wordcount/input \
  /ateliers/atelier2/wordcount/output
```

âœ… Hands-on with MapReduce workflow.

</details>

---

### 3ï¸âƒ£ Atelier 3 â€“ Pig

<details>
<summary>ğŸ– Pig Analytics</summary>

```bash
pig -x local << 'EOF'
vols = LOAD '/ateliers/atelier3/vol.csv' USING PigStorage(';') 
       AS (year:int, month:int, day:int, flight:chararray, depart:chararray, destination:chararray, distance:int);

departure_counts = FOREACH (GROUP vols BY depart) GENERATE group, COUNT(vols);
STORE departure_counts INTO '/ateliers/atelier3/pig_output/departure_counts';
quit;
EOF
```

âœ… Aggregations with Pig scripting.

</details>

---

### 4ï¸âƒ£ Atelier 4 â€“ Hive Basics

<details>
<summary>ğŸ Hive SQL</summary>

```sql
CREATE DATABASE IF NOT EXISTS atelier4;
USE atelier4;

CREATE EXTERNAL TABLE vols (
    year INT, month INT, day INT, flight STRING,
    depart STRING, destination STRING, distance INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'
LOCATION '/ateliers/atelier4/';

-- Flights per airport
SELECT depart, COUNT(*) FROM vols GROUP BY depart LIMIT 10;
```

âœ… Create & query Hive tables.

</details>

---

### 5ï¸âƒ£ Atelier 5 â€“ Hive File Formats

<details>
<summary>ğŸ“¦ Text vs Parquet</summary>

```sql
CREATE EXTERNAL TABLE bank_txt (
    id INT, age INT, sex STRING, region STRING, income DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/ateliers/atelier5/';

CREATE TABLE bank_parquet STORED AS PARQUET AS SELECT * FROM bank_txt;

-- Compare performance
SELECT region, AVG(income) FROM bank_txt GROUP BY region;
```

âœ… Compare storage formats.

</details>

---

### 6ï¸âƒ£ Atelier 6 â€“ Hive Partitioning

<details>
<summary>ğŸ“Š Partitioned Tables</summary>

```sql
CREATE TABLE bank_partitioned (
    id INT, age INT, sex STRING, income DOUBLE
)
PARTITIONED BY (region STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

-- Enable dynamic partitioning
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE bank_partitioned PARTITION(region)
SELECT id, age, sex, income, region FROM atelier5.bank_txt;
```

âœ… Partitioning for query optimization.

</details>

---

## âœ… Progress Tracker

| Atelier           | Status      |
| ----------------- | ----------- |
| HDFS              | âœ… Completed |
| MapReduce         | âœ… Completed |
| Pig               | âœ… Completed |
| Hive Basics       | âœ… Completed |
| Hive File Formats | âœ… Completed |
| Hive Partitioning | âœ… Completed |

---

<p align="center"><i>ğŸ‰ Congratulations! You now have a fully running Hadoop ecosystem with all 6 ateliers explored.</i></p>  
