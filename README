# 🚀 Big Data Ateliers – Complete Hadoop Ecosystem

<p align="center">  
  <img src="https://img.shields.io/badge/Docker-ready-blue?logo=docker"/>  
  <img src="https://img.shields.io/badge/Hadoop-3.x-yellow?logo=apache"/>  
  <img src="https://img.shields.io/badge/Hive-SQL-orange?logo=apachehive"/>  
  <img src="https://img.shields.io/badge/Pig-Scripting-lightgrey?logo=apache"/>  
</p>  

<p align="center">  
A complete **Hadoop ecosystem lab** with **6 ateliers** to practice:  
🔹 HDFS  
🔹 MapReduce  
🔹 Pig  
🔹 Hive  
🔹 File formats  
🔹 Partitioning  

All running in **Docker** for simplicity and reproducibility.

</p>  

---

## 🌐 Architecture

```mermaid
flowchart TD
    Client[💻 Client] --> NN[Namenode]
    NN -->|Stores metadata| DNs[Datanodes]
    DNs -->|Stores blocks| HDFS[(HDFS Storage)]
    HDFS --> MR[⚙️ MapReduce]
    MR --> Pig[🐖 Pig Scripts]
    MR --> Hive[🐝 Hive SQL]
```

---

## ⚡ Quick Start

```bash
# Clone repository
git clone https://github.com/your-org/hadoop-ecosystem-ateliers.git
cd hadoop-ecosystem-ateliers

# Build and start cluster
docker compose up -d --build

# Access Namenode
docker exec -it namenode bash

# Prepare ateliers in HDFS
hdfs dfs -mkdir -p /ateliers/{atelier1,atelier2,atelier3,atelier4,atelier5,atelier6}

# Upload datasets
for i in {1..6}; do
  hdfs dfs -put /ateliers_local/atelier$i/* /ateliers/atelier$i/
done
```

---

## 📚 Ateliers

### 1️⃣ Atelier 1 – HDFS Basics

<details>
<summary>🔎 Explore HDFS</summary>

```bash
# List ateliers
hdfs dfs -ls /ateliers/

# Show contents
hdfs dfs -cat /ateliers/atelier1/input/FL_insurance.csv | head -3

# Cluster report
hdfs dfsadmin -report | head -20
```

✅ Learn file navigation, inspection, and block reports.

</details>

---

### 2️⃣ Atelier 2 – MapReduce

<details>
<summary>⚙️ Run WordCount Job</summary>

```bash
# Create input/output dirs
hdfs dfs -mkdir -p /ateliers/atelier2/wordcount/{input,output}
hdfs dfs -put /ateliers/atelier2/maman.txt /ateliers/atelier2/wordcount/input/

# Compile & package
javac -cp $(hadoop classpath) -d build WordCount.java
jar -cvf wordcount.jar -C build/ .

# Run job
hadoop jar wordcount.jar WordCount \
  /ateliers/atelier2/wordcount/input \
  /ateliers/atelier2/wordcount/output
```

✅ Hands-on with MapReduce workflow.

</details>

---

### 3️⃣ Atelier 3 – Pig

<details>
<summary>🐖 Pig Analytics</summary>

```bash
pig -x local << 'EOF'
vols = LOAD '/ateliers/atelier3/vol.csv' USING PigStorage(';') 
       AS (year:int, month:int, day:int, flight:chararray, depart:chararray, destination:chararray, distance:int);

departure_counts = FOREACH (GROUP vols BY depart) GENERATE group, COUNT(vols);
STORE departure_counts INTO '/ateliers/atelier3/pig_output/departure_counts';
quit;
EOF
```

✅ Aggregations with Pig scripting.

</details>

---

### 4️⃣ Atelier 4 – Hive Basics

<details>
<summary>🐝 Hive SQL</summary>

```sql
CREATE DATABASE IF NOT EXISTS atelier4;
USE atelier4;

CREATE EXTERNAL TABLE vols (
    year INT, month INT, day INT, flight STRING,
    depart STRING, destination STRING, distance INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'
LOCATION '/ateliers/atelier4/';

-- Flights per airport
SELECT depart, COUNT(*) FROM vols GROUP BY depart LIMIT 10;
```

✅ Create & query Hive tables.

</details>

---

### 5️⃣ Atelier 5 – Hive File Formats

<details>
<summary>📦 Text vs Parquet</summary>

```sql
CREATE EXTERNAL TABLE bank_txt (
    id INT, age INT, sex STRING, region STRING, income DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/ateliers/atelier5/';

CREATE TABLE bank_parquet STORED AS PARQUET AS SELECT * FROM bank_txt;

-- Compare performance
SELECT region, AVG(income) FROM bank_txt GROUP BY region;
```

✅ Compare storage formats.

</details>

---

### 6️⃣ Atelier 6 – Hive Partitioning

<details>
<summary>📊 Partitioned Tables</summary>

```sql
CREATE TABLE bank_partitioned (
    id INT, age INT, sex STRING, income DOUBLE
)
PARTITIONED BY (region STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

-- Enable dynamic partitioning
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE bank_partitioned PARTITION(region)
SELECT id, age, sex, income, region FROM atelier5.bank_txt;
```

✅ Partitioning for query optimization.

</details>

---

## ✅ Progress Tracker

| Atelier           | Status      |
| ----------------- | ----------- |
| HDFS              | ✅ Completed |
| MapReduce         | ✅ Completed |
| Pig               | ✅ Completed |
| Hive Basics       | ✅ Completed |
| Hive File Formats | ✅ Completed |
| Hive Partitioning | ✅ Completed |

---

<p align="center"><i>🎉 Congratulations! You now have a fully running Hadoop ecosystem with all 6 ateliers explored.</i></p>  
