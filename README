# 🚀 Big Data Ateliers – Hadoop Ecosystem Labs

[![Docker](https://img.shields.io/badge/Docker-ready-blue?logo=docker)](https://www.docker.com/)
[![Hadoop](https://img.shields.io/badge/Hadoop-3.x-yellow?logo=apache)](https://hadoop.apache.org/)
[![Hive](https://img.shields.io/badge/Hive-SQL%20on%20HDFS-orange?logo=apachehive)](https://hive.apache.org/)
[![Pig](https://img.shields.io/badge/Pig-Scripting%20on%20HDFS-lightgrey)](https://pig.apache.org/)

📌 This repository contains **6 hands-on Ateliers** to explore the Hadoop ecosystem inside Docker:

* HDFS commands
* MapReduce programming
* Pig scripting
* Hive queries
* Hive file formats (Text/Parquet)
* Hive partitioning

---

## 📊 Ecosystem Architecture

```
   ┌───────────┐       ┌──────────────┐
   │   Client  │──────▶│   Namenode    │
   └───────────┘       └──────────────┘
         │                     │
         ▼                     ▼
   ┌───────────┐       ┌──────────────┐
   │ Datanodes │◀─────▶│   HDFS Data  │
   └───────────┘       └──────────────┘
         │
   ┌─────────────┐
   │ MapReduce   │  ← WordCount, custom jobs
   └─────────────┘
         │
   ┌─────────────┬─────────────┐
   │     Pig     │     Hive     │
   │ (scripts)   │ (SQL engine) │
   └─────────────┴─────────────┘
```

---

## 🛠️ Quick Setup

```bash
# Enter namenode container
docker exec -it namenode bash

# Create atelier directories
hdfs dfs -mkdir -p /ateliers/{atelier1,atelier2,atelier3,atelier4,atelier5,atelier6}

# Upload datasets
for i in {1..6}; do
  hdfs dfs -put /opt/scripts/../data/ateliers/atelier$i/* /ateliers/atelier$i/
done
```

---

## 📂 Ateliers

### 📁 Atelier 1 – HDFS Commands

<details>
<summary>🔎 Click to expand</summary>  

```bash
# List ateliers
hdfs dfs -ls /ateliers/

# Show file contents
hdfs dfs -cat /ateliers/atelier1/input/FL_insurance.csv | head -3

# HDFS report
hdfs dfsadmin -report | head -20
```

✅ Covers: HDFS navigation, file ops, block info.

</details>  

---

### 📁 Atelier 2 – MapReduce

<details>
<summary>📝 WordCount Job</summary>  

```bash
hdfs dfs -mkdir -p /ateliers/atelier2/wordcount/{input,output}
hdfs dfs -put /ateliers/atelier2/maman.txt /ateliers/atelier2/wordcount/input/

# Compile WordCount.java
javac -cp $(hadoop classpath) -d build WordCount.java
jar -cvf wordcount.jar -C build/ .

# Run job
hadoop jar wordcount.jar WordCount \
  /ateliers/atelier2/wordcount/input \
  /ateliers/atelier2/wordcount/output
```

✅ Covers: MapReduce basics, WordCount results.

</details>  

---

### 📁 Atelier 3 – Pig

<details>
<summary>🐖 Pig Analytics</summary>  

```bash
pig -x local << 'EOF'
vols = LOAD '/ateliers/atelier3/vol.csv' USING PigStorage(';') 
       AS (year:int, month:int, day:int, flight:chararray, depart:chararray, destination:chararray, distance:int);

departure_counts = FOREACH (GROUP vols BY depart) GENERATE group, COUNT(vols);
STORE departure_counts INTO '/ateliers/atelier3/pig_output/departure_counts';
quit;
EOF
```

✅ Covers: Pig scripting, grouping, aggregations.

</details>  

---

### 📁 Atelier 4 – Hive Basics

<details>
<summary>🐝 SQL on Hadoop</summary>  

```sql
CREATE DATABASE IF NOT EXISTS atelier4;
USE atelier4;

CREATE EXTERNAL TABLE vols (
    year INT, month INT, day INT, flight STRING,
    depart STRING, destination STRING, distance INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'
LOCATION '/ateliers/atelier4/';

-- Query: flights per airport
SELECT depart, COUNT(*) FROM vols GROUP BY depart LIMIT 10;
```

✅ Covers: Hive tables, queries, aggregations.

</details>  

---

### 📁 Atelier 5 – Hive File Formats

<details>
<summary>📦 Text vs Parquet</summary>  

```sql
CREATE EXTERNAL TABLE bank_txt (
    id INT, age INT, sex STRING, region STRING, income DOUBLE
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/ateliers/atelier5/';

CREATE TABLE bank_parquet STORED AS PARQUET AS SELECT * FROM bank_txt;

-- Compare queries
SELECT region, AVG(income) FROM bank_txt GROUP BY region;
```

✅ Covers: Storage optimization, Parquet vs Text.

</details>  

---

### 📁 Atelier 6 – Hive Partitioning

<details>
<summary>📊 Partitioned Tables</summary>  

```sql
CREATE TABLE bank_partitioned (
    id INT, age INT, sex STRING, income DOUBLE
)
PARTITIONED BY (region STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

-- Load with dynamic partitioning
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE bank_partitioned PARTITION(region)
SELECT id, age, sex, income, region FROM atelier5.bank_txt;
```

✅ Covers: Partitioning, query optimization.

</details>  

---

## ✅ Final Checklist

* [x] HDFS commands (Atelier 1)
* [x] MapReduce WordCount (Atelier 2)
* [x] Pig analytics (Atelier 3)
* [x] Hive basics (Atelier 4)
* [x] Hive file formats (Atelier 5)
* [x] Hive partitioning (Atelier 6)

🎉 All ateliers completed successfully!

---

🔗 **Tip:** You can re-run any atelier independently by entering the container:

```bash
docker exec -it namenode bash
```

---
